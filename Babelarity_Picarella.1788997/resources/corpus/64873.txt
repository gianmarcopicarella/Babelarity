In mathematics, nonlinear programming is the process of solving an optimization problem defined by a system of equalities and inequalities, collectively termed constraints, over a set of unknown real variables, along with an objective function to be maximized or minimized, where some of the constraints or the objective function are nonlinear. It is the sub-field of mathematical optimization that deals with problems that are not linear.
A typical nonconvex problem is that of optimising transportation costs by selection from a set of transportation methods, one or more of which exhibit economies of scale, with various connectivities and capacity constraints. An example would be petroleum product transport given a selection or combination of pipeline, rail tanker, road tanker, river barge, or coastal tankship. Owing to economic batch size the cost functions may have discontinuities in addition to smooth changes.
Modern engineering practice involves much numerical optimization. Except in certain narrow but important cases such as passive electronic circuits, engineering problems are typically nonlinear, and they are usually very complicated. Nonlinear optimization methods are used in engineering, for example, to construct computational models of oil reservoirs, or for decision making in subsurface oil and gas reservoir projects.
In experimental science, some simple data analysis (such as fitting a spectrum with a sum of peaks of known location and shape but unknown magnitude) can be done with linear methods, but in general these problems, also, are nonlinear. Typically, one has a theoretical model of the system under study with variable parameters in it and a model the experiment or experiments, which may also have unknown parameters. One tries to find a best fit numerically. In this case one often wants a measure of the precision of the result, as well as the best fit itself.
Let n, m, and p be positive integers. Let X be a subset of Rn, let f, gi, and hj be real-valued functions on X for each i in {1, …, m} and each j in {1, …, p}.
A nonlinear minimization problem is an optimization problem of the form
                  minimize 
                f
                (
                x
                )
                  subject to 
                  g
                    i
                (
                x
                )
                ≤
                0
                   for each 
                i
                ∈
                {
                1
                ,
                …
                ,
                m
                }
                  h
                    j
                (
                x
                )
                =
                0
                   for each 
                j
                ∈
                {
                1
                ,
                …
                ,
                p
                }
                x
                ∈
                X
                .
    {\displaystyle {\begin{aligned}{\text{minimize }}&f(x)\\{\text{subject to }}&g_{i}(x)\leq 0{\text{ for each }}i\in \{1,\dotsc ,m\}\\&h_{j}(x)=0{\text{ for each }}j\in \{1,\dotsc ,p\}\\&x\in X.\end{aligned}}}
A nonlinear maximization problem is defined in a similar way.
There are several possibilities for the nature of the constraint set, also known as the feasible set or feasible region.
An infeasible problem is one for which no set of values for the choice variables satisfies all the constraints. That is, the constraints are mutually contradictory, and no solution exists.
A feasible problem is one for which there exists at least one set of values for the choice variables satisfying all the constraints.
An unbounded problem is a feasible problem for which the objective function can be made to be better than any given finite value. Thus there is no optimal solution, because there is always a feasible solution that gives a better objective function value than does any given proposed solution.
If the objective function f is linear and the constrained space is a polytope, the problem is a linear programming problem, which may be solved using well-known linear programming techniques such as the simplex method.
If the objective function is concave (maximization problem), or convex (minimization problem) and the constraint set is convex, then the program is called convex and general methods from convex optimization can be used in most cases.
If the objective function is quadratic and the constraints are linear, quadratic programming techniques are used.
If the objective function is a ratio of a concave and a convex function (in the maximization case) and the constraints are convex, then the problem can be transformed to a convex optimization problem using fractional programming techniques.
Several methods are available for solving nonconvex problems. One approach is to use special formulations of linear programming problems. Another method involves the use of branch and bound techniques, where the program is divided into subclasses to be solved with convex (minimization problem) or linear approximations that form a lower bound on the overall cost within the subdivision. With subsequent divisions, at some point an actual solution will be obtained whose cost is equal to the best lower bound obtained for any of the approximate solutions. This solution is optimal, although possibly not unique. The algorithm may also be stopped early, with the assurance that the best possible solution is within a tolerance from the best point found; such points are called ε-optimal. Terminating to ε-optimal points is typically necessary to ensure finite termination. This is especially useful for large, difficult problems and problems with uncertain costs or values where the uncertainty can be estimated with an appropriate reliability estimation.
Under differentiability and constraint qualifications, the Karush–Kuhn–Tucker (KKT) conditions provide necessary conditions for a solution to be optimal. Under convexity, these conditions are also sufficient. If some of the functions are non-differentiable, subdifferential versions of Karush–Kuhn–Tucker (KKT) conditions are available.
A simple problem can be defined by the constraints
x1 ≥ 0
x2 ≥ 0
x12 + x22 ≥ 1
x12 + x22 ≤ 2
with an objective function to be maximized
f(x) = x1 + x2
where x = (x1, x2).
Another simple problem can be defined by the constraints
x12 − x22 + x32 ≤ 2
x12 + x22 + x32 ≤ 10
with an objective function to be maximized
f(x) = x1x2 + x2x3
where x = (x1, x2, x3).
Curve fitting
Least squares minimization
Linear programming
nl (format)
List of optimization software
Quadratically constrained quadratic programming
Werner Fenchel, who created the foundation for nonlinear programming
